{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFc6x4ExtjfmxKGcYkKIym",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/3ambro/linkin-park-text-analysis/blob/main/Linkin_Park_Text_Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Why Am I Text Cleaning In Colab?**\n",
        "Voyant Tools is, for some reason, not asccurately removing stopwords when imported into their tools. For this reason, instead of continuously troublshooting until it works solely within Voyant, I've decided to instead use Google Colab/Python to create 'clean' versions of each text file using my custom stopword list. This code is partially written with ChatGPT assistance."
      ],
      "metadata": {
        "id": "P1BwzUfOYMUs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MKsUyJtXs6K",
        "outputId": "d5f08a6f-0df3-48e1-f0f5-f48c57eb7f22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "2SOZwJ5hlNHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "ssopcwk9nI-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dir = '/content/drive/MyDrive/Linkin Park Text Files/Raw Texts'\n",
        "output_dir = '/content/drive/MyDrive/Linkin Park Text Files/Cleaned Texts'\n",
        "stopwords_file = '/content/drive/MyDrive/Linkin Park Text Files/stopwords.txt'"
      ],
      "metadata": {
        "id": "6T0PNSiR05ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)"
      ],
      "metadata": {
        "id": "fhntxLIdphLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function for text cleaning\n",
        "def clean_text(text, custom_stopwords):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    cleaned_tokens = [token for token in tokens if token not in custom_stopwords]\n",
        "    cleaned_text = ' '.join(cleaned_tokens)\n",
        "    return cleaned_text\n",
        "\n",
        "# Read custom stopwords from file\n",
        "def read_stopwords(stopwords_file):\n",
        "    with open(stopwords_file, 'r') as file:\n",
        "        stopwords = [word.strip() for word in file.readlines()]\n",
        "    return stopwords\n",
        "\n",
        "# Read custom stopwords\n",
        "custom_stopwords = read_stopwords(stopwords_file)\n",
        "\n",
        "# Iterate over files in input directory\n",
        "for filename in os.listdir(input_dir):\n",
        "    if filename.endswith('.txt'):  # Process only text files\n",
        "        input_path = os.path.join(input_dir, filename)\n",
        "        output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "        # Read input file\n",
        "        with open(input_path, 'r') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        # Clean text\n",
        "        cleaned_text = clean_text(text, custom_stopwords)\n",
        "\n",
        "        # Write cleaned text to output file\n",
        "        with open(output_path, 'w') as file:\n",
        "            file.write(cleaned_text)"
      ],
      "metadata": {
        "id": "T06MUeGAoDVp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}